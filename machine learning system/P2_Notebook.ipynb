{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Part A","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:53:24.781468Z","iopub.execute_input":"2022-01-19T10:53:24.782145Z","iopub.status.idle":"2022-01-19T10:53:24.786997Z","shell.execute_reply.started":"2022-01-19T10:53:24.782108Z","shell.execute_reply":"2022-01-19T10:53:24.785065Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd #data pre-processing\nimport matplotlib #data visualization \nimport matplotlib.pyplot as plt #data visualization \nimport seaborn as sns #data visualization \nimport missingno as msno #Missing value interpretation\nimport shap\n\nfrom IPython.display import display\n\n\n\nfrom sklearn.model_selection import KFold,RepeatedStratifiedKFold, cross_val_score, GridSearchCV #Cross validation\nfrom sklearn.model_selection import train_test_split #train_test_split splits dataset to training and test data and able to randomize the data\n\n#feature scaling and label encoding\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,LabelEncoder \n\n#Evaluation metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import log_loss \nfrom sklearn.metrics import roc_auc_score\n\n\n#Pipeline to assemble several steps that can be cross-validated together while setting different parameters\nfrom sklearn.pipeline import Pipeline\n\n#Set of different classifiers for the analysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier,Pool\nimport lightgbm as lgb\nfrom sklearn import svm\n\n\n#data immuputaion methods\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\nfrom collections import Counter\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:53:24.965473Z","iopub.execute_input":"2022-01-19T10:53:24.966256Z","iopub.status.idle":"2022-01-19T10:53:24.974859Z","shell.execute_reply.started":"2022-01-19T10:53:24.966195Z","shell.execute_reply":"2022-01-19T10:53:24.974163Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Predictor class that compares different classifiers and find the best classifier for the given dataset.","metadata":{"_uuid":"23c238f4-bac5-4a31-b5ad-5e8ec5eca438","_cell_guid":"8d4e8e35-2ef6-4684-8121-25a22f2c59f7","trusted":true}},{"cell_type":"code","source":"class predictor:\n    data = pd.DataFrame()\n    imp = ''\n    scalingStrategy = ''\n    pred = ''\n    \n    def __init__(self,data, imp='missing', strategy = 'standard'):\n        #receives data and makes copy to avoid working on original data\n        self.data = data.copy(deep=True)\n        self.imp = imp\n        self.scalingStrategy = strategy\n        \n        \n    def visualizeResults(self,best_model):#Visualizes the classifiers results for comparison\n        clfs = []\n        result    = pd.DataFrame(best_model.cv_results_)\n#        sort models based on rank after cross validation\n        result    = result.sort_values('rank_test_score')\n    \n        m = [['CatBoostClassifier','CatBoost'],['XGBClassifier','XGB'],\n             ['RandomForestClassifier','RandomForest'],['DecisionTreeClassifier','DT']\n             ,['LGBMClassifier','LGBM'],['SVC','LSVM']]\n        \n        for i in result['param_classifier']:\n            for j in range(len(m)):\n                if m[j][0] == type(i).__name__:\n                    clfs.append(m[j][1])  \n    \n#        plot comparison line graph of different classification models\n        data_plot = pd.DataFrame({\"Classifier\": clfs,\"Results\":result['mean_test_score']})\n        sns.lineplot(x = \"Classifier\", y = \"Results\", data=data_plot)\n        plt.title('Classifiers comparative analysis')\n        plt.show()\n        \n        \n    def immputation(self,data): #immpute dataframe using different immputation stratigies \n        feature =  data.loc[:, self.data.columns != 'Class']\n        target  =  data.loc[:, 'Class']\n        \n        if self.imp == 'missing': #remove the specified column\n            feature.drop(['F21'], axis = 1, inplace = True) \n        elif self.imp == 'mean': #replace the empty values with mean of that column\n            feature.fillna(df.F21.mean(), inplace=True) \n        elif self.imp == 'iterative': #A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.\n            imp = IterativeImputer() \n            feature = imp.fit_transform(feature) \n        elif self.imp == 'knn': #Each sampleâ€™s missing values are imputed using the mean value from n_neighbors nearest neighbors \n            knn_imp = KNNImputer(n_neighbors=3)\n            feature = knn_imp.fit_transform(feature)\n#         chaining function calls feature scaling methos \n        return self.featureScaling(feature,target)\n    \n    def featureScaling(self, feature, target): #Normalise data using specified strategy\n        feature_temp = ''\n        if self.scalingStrategy == 'minmax': #Transform features by scaling each feature to a given range.\n            scaler = MinMaxScaler().fit(feature)\n            MinMaxScaler()\n            feature_temp = scaler.transform(feature)\n        elif self.scalingStrategy == 'standard': #Standardize features by removing the mean and scaling to unit variance.  \n            scaler = StandardScaler().fit(feature)\n            StandardScaler()\n            feature_temp = scaler.transform(feature)\n        #Target encoding\n        target = LabelEncoder().fit_transform(target) #Label encoding target value\n        return (feature_temp, target) \n    \n    def visualizeResult(self, result, best_model): #Visualizing results obtained using best model\n        dic    = dict(Counter(pd.Series(result)))\n        dicTemp = dic.items()\n        df = pd.DataFrame(dicTemp, columns=['Target', 'Count'])\n        df.plot.bar(x='Target', y='Count', rot=0, color={'#C3553A','#76A3B1'})\n        diclist = list(dicTemp)\n        print('\\n')\n        print('Number of profitable hotels : ', diclist[0][1])\n        print('Number of hotels that may not be profitable : ', diclist[1][1])\n        print('\\n')\n\n\n    def predict(self,best_model): #prediction using test data\n        #read test dataset\n        test      = pd.read_csv('CE802_P2_Test.csv')\n        #perform imputation and feature scaling on the test set\n        X_test    = self.immputation(test)[0]\n        #predict the target value for the given test features\n        y_pred    = best_model.predict(X_test)\n        self.pred = y_pred\n        #calls visualizeResult to visualize obtained results\n        self.visualizeResult(y_pred, best_model)\n\n                \n        \n    def modelFinder(self, clfs): #compares and finds the best model \n        #Performs immputation and feature scaling on the training set\n        immputed = self.immputation(self.data)\n        feature, target = immputed[0], immputed[1]\n        \n        #Classifiers pipeline params of chosen classifiers\n        params = {\n                 'catboost' : \n                        {\"classifier\": [CatBoostClassifier(verbose=False)],\n                  'classifier__learning_rate':[0.01,0.05, 0.10, 0.15, 0.20, 0.25, 0.3]\n                   ,'classifier__iterations' : [10,100,500,750,1000]\n#                          ,\"classifier__n_estimators\":[10, 100,200, 1000]\n                        , \"classifier__max_depth\" : [3,5,7,9,10,15,20,25],\n                  },\n                  'xgb' : {\"classifier\": [XGBClassifier(random_state = 42,learning_rate = 0.05,\n                                                max_depth = 6,eval_metric='mlogloss',\n                                                min_child_weight=1,gamma=0.0,\n                                                colsample_bytree = '0.7' )]},\n                  'randomforest' : {\"classifier\": [RandomForestClassifier()],\n                 'classifier__max_depth' : (3,5,7,9,10,15,20,25),\n                 \"classifier__n_estimators\":[10, 100, 1000],\n                 \"classifier__bootstrap\" : [True, False]\n                 },\n    \n                'dt' : {\"classifier\": [DecisionTreeClassifier()],\n                'classifier__max_depth' : (3,5,7,9,10,15,20,25),\n                  'classifier__criterion' : ('gini', 'entropy')\n              , 'classifier__min_samples_split' : (2,4,6)\n                 },\n                'svm' : {\"classifier\": [svm.SVC()],\n                  'classifier__C': [0.1, 1, 10, 100, 1000],\n                  'classifier__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n                  'classifier__kernel': ['linear','rbf']\n                },\n    \n                'lgbm' : {\"classifier\": [lgb.LGBMClassifier()],\n                 \"classifier__learning_rate\"    : [0.01,0.05, 0.10, 0.15, 0.20, 0.25, 0.3 ] , \n#1                 'classifier__bagging_fraction': [0.5, 0.8],\n                \"classifier__objective\":[\"binary\"],\n                       \"classifier__ bagging_freq\":[5,8,10],   \n# 2                'classifier__feature_fraction': [0.5, 0.8],\n                'classifier__max_depth': [3,5,7,9,10,15,20,25],\n                'classifier__min_data_in_leaf': [90, 120],\n                'classifier__num_leaves': [1200, 1550]\n                }}\n        #Train test spit with 80% for training and 20% for testing.\n        X_train,X_test,y_train,y_test=train_test_split(feature,target,test_size=0.2,random_state=0)\n        #Create pipeline  that Sequentially applys a list of transforms and a final estimator\n        pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n        # Create dictionary with candidate learning algorithms and their hyperparameters\n        grid_param = [params[i] for i in clfs ]   \n        # create a gridsearch of the pipeline, the fit the best model\n        gridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1) # Fit grid search\n        best_model = gridsearch.fit(X_train,y_train)\n        print(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))\n        print('\\n')\n        \n        #perform prediction on test data \n        y_pred = best_model.predict(X_test)\n        cm = confusion_matrix(y_test,y_pred)\n        ax = sns.heatmap(cm, annot=True, cmap='Blues')\n\n        ax.set_title('Confusion Matrix \\n\\n');\n        ax.set_xlabel('\\nPredicted Values')\n        ax.set_ylabel('Actual Values ');\n\n        ## Ticket labels - List must be in alphabetical order\n        ax.xaxis.set_ticklabels(['False','True'])\n        ax.yaxis.set_ticklabels(['False','True'])\n\n        ## Display the visualization of the Confusion Matrix.\n        print('\\n')\n        plt.show()\n        print(classification_report(y_test,y_pred))\n        print('log loss : ', log_loss(y_test, y_pred, eps=1e-15))\n        print('\\n')\n        print('roc_auc_score :',roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1]))\n        print('\\n')\n   \n        self.visualizeResults(best_model)\n        self.predict(best_model)\n        # Create a dataframe of feature importance\n        print('\\n')\n        if type(best_model.best_estimator_._final_estimator).__name__ == 'CatBoostClassifier':\n            #calculate feature importace of the CatBoost Classifier\n            df_feature_importance = pd.DataFrame(best_model.best_estimator_._final_estimator.get_feature_importance(prettified=True))\n            #plotting feature importance\n            plt.figure(figsize=(12, 6));\n            feature_plot= sns.barplot(x=\"Importances\", y=\"Feature Id\", data=df_feature_importance,palette=\"cool\");\n            plt.title('Feature importance');\n            \n#             shap_values = best_model.best_estimator_._final_estimator.get_feature_importance(Pool(X_test, label=y_test,cat_features=categorical_features_indices), \n#                                                                      type=\"ShapValues\")\n#             expected_value = shap_values[0,-1]\n#             shap_values = shap_values[:,:-1]\n\n#             shap.initjs()\n#             shap.force_plot(expected_value, shap_values[3,:], X_test.iloc[3,:])","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:56:10.184006Z","iopub.execute_input":"2022-01-19T10:56:10.184298Z","iopub.status.idle":"2022-01-19T10:56:10.218654Z","shell.execute_reply.started":"2022-01-19T10:56:10.184255Z","shell.execute_reply":"2022-01-19T10:56:10.217869Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Main class of the machine learning system which includes seperate functions for each stage of data-preprocessing","metadata":{}},{"cell_type":"code","source":"import missingno as msno \nimport seaborn as sns\n\nclass Mlsystem: #Mlsytem main class of the ananlysis\n    df = ''\n    df_test = pd.DataFrame() \n    df_org = ''\n    predicted = ''\n    \n    def __init__(self):\n        #Reads both train and test set for the experiment\n        self.df      = pd.read_csv('CE802_P2_Data.csv')\n        self.df_test = pd.read_csv('CE802_P2_Test.csv')\n        #makes copy to avoid making changes to original dataset\n        self.df_org  = self.df.copy(deep=True) \n        \n    def featureEngineering(self): #Feature engineering procedures\n        \n        display(self.df.head())\n        \n        print('Columns Data type and Null Counter \\n')\n        print(self.df.info()) # column wise information of the DataFrame\n        \n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n        \n        print('Rows * columns of training set : ',self.df.shape) # shape of the dataframe\n        \n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n        \n        print('Description of given data \\n')\n        t = self.df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f'))).transpose()\n        display(t.head().T)\n       \n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n#         to check the balance of the target column\n        print('Target balance check    : ', self.df.Class.unique(),'\\n' ) # values of the Target suggest that it is a binary classification problem\n        plt.figure()\n        \n        #Bar chart representation of the target column's balance\n        sns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [8, 5]})\n        sns.distplot(\n        self.df['Class'], norm_hist=False, kde=False, bins=20, hist_kws={\"alpha\": 1}\n        ).set(xlabel='Target/Class', ylabel='Count'); #data is balanced   \n        plt.show()\n        plt.figure()\n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n        \n#       NULL value detection\n        print('Missing value indicator : ', self.df.isna().any().any()) # missing value detection\n        \n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n        \n#       empty value detection\n        print('Null value indicator    : ', self.df.isnull().values.any()) #Checking presence of empty values\n        \n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n        \n#       uses bar char to find frequeny of the missing value\n        print('Missing value identification using bar chart \\n')\n        msno.bar(self.df)\n        plt.figure()\n        plt.show()\n        print('Above heatmap clearly shows that 50% of the F21 is missing')\n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n\n        \n#       barchart representation of each feature for interpretation  \n        print('Check Data spread using Histograms \\n')\n        self.df.hist(bins=30, figsize=(20, 15))\n        plt.figure()\n        plt.show()\n\n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n        \n#       heatmap to check missing value spread accross the feature  \n        print('Missing value spread accross the column \\n')\n        sns.heatmap(self.df.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")\n        plt.show()\n        \n\n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n\n#         Check null value percentage of the feature F21\n        F21NullValuesPercentage = (self.df['F21'].isnull().sum()/self.df.shape[0]*100).round(2) # Output: Percentage of missing values\n        print('F21 Null value percentage :' , F21NullValuesPercentage,'%')\n\n        print('\\n-------------------------------------------------------------------------------------------------------------\\n')\n             \n        \n    def featureSelection(self): #To carry out feature selection procedures\n        start = \"\\033[1m\"\n        end = \"\\033[0;0m\"        \n        # calculate correlation matrix\n        print(start+'Correlation matrix heatmap'+end+'\\n'.center(200))\n        corr = self.df.corr()# plot the heatmap\n        fig, ax = plt.subplots(figsize=(20,20))         # Sample figsize in inches\n        sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))\n        plt.show()\n#       Outlier detection using box plot\n        print('\\n')\n        fig, ax = plt.subplots(figsize=(15,5)) \n        print(start+'Outlier detection using Box plot'+end+'\\n'.center(110))\n        self.df.boxplot()\n        plt.show()\n    \n    def modelSelection(self, models, imp='missing', scalingStrategy='standard'):\n        #Creates predictor instance to compare and find best model     \n        pred = predictor(self.df,imp='iterative',strategy='standard',)\n        #compare and find best model and predict results\n        pred.modelFinder(models)\n        #gets predicted values from the predictor and store it on instance variable\n        self.predicted = pred.pred\n    \n#     def dataCollection(self):\n#         pass","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:56:10.770193Z","iopub.execute_input":"2022-01-19T10:56:10.770488Z","iopub.status.idle":"2022-01-19T10:56:10.792679Z","shell.execute_reply.started":"2022-01-19T10:56:10.770457Z","shell.execute_reply":"2022-01-19T10:56:10.791683Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#Choose from List of models for the experiment\nmodels = ['catboost', 'dt', 'randomforest', 'svm', 'lgbm', 'xgb']\n#Choose between different immputation methods\nimpStrategies = ['missing','mean','iterative','knn']\n#Choose from different feature scaling methods\nscalingStrategies = ['standard', 'minmax']\n#Creates instance of Mlsystem class\nml = Mlsystem()\n#Feature Engineering\nml.featureEngineering()\n#Feature selection\nml.featureSelection()\n#creates instance of predict class to compare and find best classifier for prediction then \n# uses the best model to predict and store results on the instance variable self.predicted\nml.modelSelection(models, imp=impStrategies[3], scalingStrategy=scalingStrategies[0] )","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:56:11.188009Z","iopub.execute_input":"2022-01-19T10:56:11.188709Z","iopub.status.idle":"2022-01-19T10:56:54.507927Z","shell.execute_reply.started":"2022-01-19T10:56:11.188669Z","shell.execute_reply":"2022-01-19T10:56:54.507270Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Get predicted values and convert them into a list \nval = ml.predicted.tolist()\n# convert result list to a dataframe\nfinal_result = pd.DataFrame({'Class':val})\n# inverse label encoding\nfinal_result.replace(1,True, inplace=True)\nfinal_result.replace(0,False, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:53:19.932555Z","iopub.status.idle":"2022-01-19T10:53:19.933152Z","shell.execute_reply.started":"2022-01-19T10:53:19.932930Z","shell.execute_reply":"2022-01-19T10:53:19.932953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Part B","metadata":{"_uuid":"9bde8e0d-1f4b-448e-a79a-9af31b08de0b","_cell_guid":"91ec51a3-6a9b-4dc3-b7d3-3fc4905fc3a1","trusted":true}},{"cell_type":"code","source":"# HERE YOU WILL USE THIS TEMPLATE TO SAVE THE PREDICTIONS ON THE TEST SET\n\n# Load the test data\ntest_df = pd.read_csv('CE802_P2_Test.csv')\n\n# Make sure you work on a copy\ntest_data = test_df.iloc[:,:-1].copy()\n\npredicted = final_result # CHANGE HERE -- use your previously trained predictor and apply it to test_data\n                # (test_data can be modified if needed but make sure you don't change the order of the rows)...\n\n# Replace the last (empty) column with your prediction\ntest_df.iloc[:,-1] = predicted\n\n# Save to the destination file\ntest_df.to_csv('CE802_P2_Test_Predictions.csv', index=False, float_format='%.8g')\n\n# IMPORTANT!! Make sure only the last column has changed\nassert pd.read_csv('CE802_P2_Test.csv').iloc[:,:-1].equals(pd.read_csv('CE802_P2_Test_Predictions.csv').iloc[:,:-1])","metadata":{"_uuid":"48185e20-b2d6-4d34-8481-2f301c4720a4","_cell_guid":"0d883c8e-68ad-4dd7-99c6-87f7b0513e5d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-19T10:53:19.936103Z","iopub.status.idle":"2022-01-19T10:53:19.936709Z","shell.execute_reply.started":"2022-01-19T10:53:19.936486Z","shell.execute_reply":"2022-01-19T10:53:19.936511Z"},"trusted":true},"execution_count":null,"outputs":[]}]}